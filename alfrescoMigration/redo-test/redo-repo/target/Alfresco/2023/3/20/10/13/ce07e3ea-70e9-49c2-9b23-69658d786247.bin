
Long�Short�Term
Memory

Ki Hyun Kim

nlp.with.deep.learning@gmail.com




Gradient�Vanishing�in�Vanilla�RNN

• RNN 내부에는 tanh가 있으므로, time-step이 길어짐에따라,
gradient�vanishing이 발생함

• 따라서긴시퀀스는학습이어려움



Gate�using�Sigmoid

• Sigmoid는 0과 1사이의값을반환하므로,
sigmoid를 곱하면마치문을열고닫는듯한효과를낼수있음.

𝑦 = 𝜎 𝑥 ×𝑓 𝑥



Long�Short�Term�Memory�(LSTM)



Long�Short�Term�Memory�(LSTM)

Resolve�vanishing�gradient�problem



Gated�Recurrent�Unit�(GRU)��[Cho�et�al.,2014]



Summary

• LSTM�vs�GRU?
• 실제로는 LSTM이좀더널리쓰이는추세. (딱히이유없음)

• LSTM은 vanilla�RNN에비해서훨씬많은파라미터를가짐
• 따라서더많은학습데이터와학습시간이필요

• 비록 LSTM이 Gradient�Vanishing�문제를해결하였지만, 무조건긴데이터를모
두기억할수있는것은아님

• Network�capacity의 한계는존재함
• Attention을 통해이를해결할수있음


