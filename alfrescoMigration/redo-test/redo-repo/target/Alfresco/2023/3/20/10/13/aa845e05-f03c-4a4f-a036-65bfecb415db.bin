
Dimension�Reduction
Ki Hyun Kim

nlp.with.deep.learning@gmail.com




Review:�Curse�of�Dimensionality

• 차원이높을수록데이터는희소하게분포하게되어학습이어려워진다.
• 모든점들을학습하기위해서, 모든구역들을살펴보아야함
• 같은구역내의점들은서로구별할수없음



Example:�MNIST

• MNIST:�28�*�28�=�784�dimension
Value between 0 and 255.

Both values are 0.

Both values are around 255.



Linear�Dimension�Reduction:�PCA

• n차원의공간에샘플들의분포가주어져있을때,
분포를잘설명하기위한새로운 axis를 찾아내는과정



Linear�Dimension�Reduction:�PCA

• 새로운 axis는 두가지조건을만족해야한다.

빨간점 사이의
거리의 합이
최대가 되도록

검은점과 검은 선
사이 거리의 합이
최소가 되도록



Linear�Dimension�Reduction:�PCA

• 새롭게찾아낸 axis에 샘플들을투사(projection)하면차원축소가가능



Why�we�need�dimension�reduction?

• Binary�Classification�in�2-D



Limitation�of�Linear�Dimension�Reduction

• Binary�Classification�in�2-D


