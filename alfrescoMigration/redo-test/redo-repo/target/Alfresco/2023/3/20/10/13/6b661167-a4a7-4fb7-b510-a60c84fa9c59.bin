
Maximum�Likelihood�
Estimation:
Equations

Ki Hyun Kim

nlp.with.deep.learning@gmail.com




MLE�Equations 𝐷 = 𝑥!, 𝑦! !"#$
&𝜃 = argmax%∈' -!"#$ log 𝑃 𝑦!|𝑥!; 𝜃= argmin%∈' −-!"#$ log 𝑃 𝑦!|𝑥!; 𝜃𝜃 ← 𝜃 − 𝛼∇%ℒ 𝜃



Connection�to�Deep�Neural�Networks

• We�can�consider�softmax result�to�probability�distribution.
Softmax



Connection�to�Deep�Neural�Networks

&𝜃 = argmin%∈' −-!"#$ log 𝑃 𝑦!|𝑥!; 𝜃
=𝑦! = 𝑓% 𝑥!

𝐷 = 𝑥!, 𝑦! !"#$

−-!"#$ log 𝑃 𝑦!|𝑥!; 𝜃 = −-!"#$ 𝑦!( ? log =𝑦!
By�implement



MLE(NLL)�and�Cross�Entropy�Loss

• Minimizing�Negative�Log-Likelihood�is�equal�to�Minimizing�Cross�Entropy.

CE 𝑦#:$, =𝑦#:$ = − 1𝑁-!"#$ -*"#+ 𝑦!,* log =𝑦!,*= − 1𝑁-!"#$ 𝑦!( ? log =𝑦! ,where 𝑦#:$ ∈ ℝ$×+, =𝑦#:$ ∈ ℝ$×+.


