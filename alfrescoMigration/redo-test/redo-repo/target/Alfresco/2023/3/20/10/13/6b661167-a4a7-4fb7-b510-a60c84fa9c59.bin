
Maximumï¿½Likelihoodï¿½
Estimation:
Equations

Ki Hyun Kim

nlp.with.deep.learning@gmail.com




MLEï¿½Equations ğ· = ğ‘¥!, ğ‘¦! !"#$
&ğœƒ = argmax%âˆˆ' -!"#$ log ğ‘ƒ ğ‘¦!|ğ‘¥!; ğœƒ= argmin%âˆˆ' âˆ’-!"#$ log ğ‘ƒ ğ‘¦!|ğ‘¥!; ğœƒğœƒ â† ğœƒ âˆ’ ğ›¼âˆ‡%â„’ ğœƒ



Connectionï¿½toï¿½Deepï¿½Neuralï¿½Networks

â€¢ Weï¿½canï¿½considerï¿½softmax resultï¿½toï¿½probabilityï¿½distribution.
Softmax



Connectionï¿½toï¿½Deepï¿½Neuralï¿½Networks

&ğœƒ = argmin%âˆˆ' âˆ’-!"#$ log ğ‘ƒ ğ‘¦!|ğ‘¥!; ğœƒ
=ğ‘¦! = ğ‘“% ğ‘¥!

ğ· = ğ‘¥!, ğ‘¦! !"#$

âˆ’-!"#$ log ğ‘ƒ ğ‘¦!|ğ‘¥!; ğœƒ = âˆ’-!"#$ ğ‘¦!( ? log =ğ‘¦!
Byï¿½implement



MLE(NLL)ï¿½andï¿½Crossï¿½Entropyï¿½Loss

â€¢ Minimizingï¿½Negativeï¿½Log-Likelihoodï¿½isï¿½equalï¿½toï¿½Minimizingï¿½Crossï¿½Entropy.

CE ğ‘¦#:$, =ğ‘¦#:$ = âˆ’ 1ğ‘-!"#$ -*"#+ ğ‘¦!,* log =ğ‘¦!,*= âˆ’ 1ğ‘-!"#$ ğ‘¦!( ? log =ğ‘¦! ,where ğ‘¦#:$ âˆˆ â„$Ã—+, =ğ‘¦#:$ âˆˆ â„$Ã—+.


