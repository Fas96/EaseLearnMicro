
Summary
Ki Hyun Kim

nlp.with.deep.learning@gmail.com




• e.g.�Tabular�Data,�Image • e.g.�Sequential�Data,�Time-series

Recurrent�Neural�Networks

Previous�methods Recurrent�Neural�Networks

𝑥 𝑦 𝑥! ℎ!

𝑦



Tensor�Shapes

• Single�layer�RNN에서 hidden�state는 곧 output이다.

• Multi-layered�RNN에서
• Output은 마지막 layer의 모든 time-step의 hidden�state이다.
• Hidden�state는 마지막 time-step의 모든 layer의 hidden�state이다.

• Bi-directional�RNN에서
• Output은 hidden�state가 2배가된다.
• Hidden�state는 layer의 갯수가 2배가된다.



BPTT

• 여러경로로 feed-forward�될 경우, back-propagation 할 때,
최종 gradient 값은각경로의 gradient들의총합이된다.

• RNN의경우에도각 time-step에 따라 feed-forward되므로,
각 경로로부터전달되어온 gradient들이더해진다.

• 따라서 time-step의 갯수만큼레이어가깊어진것과마찬가지이므로,
tanh를 활성함수로사용하는 RNN은 gradient�vanishing이 발생한다.



Applications

!" !# !$ !%
&'

!"
&'" &'# &'$ &'%

&'" &'# &'$ !" !# !$
&'" &'# &'$

&'" &'#&'(

Many to One

One to Many

Many to Many

!"
&'" &'# &'$ &'%

!# !$ !%

NLG, Machine Translation

POS Tagging, MRC

Text Classification

Architecture ApplicationsType


