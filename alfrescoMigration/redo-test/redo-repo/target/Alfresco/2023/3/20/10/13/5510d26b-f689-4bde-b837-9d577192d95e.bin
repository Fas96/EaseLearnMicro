
Hiddenï¿½Representation
Ki Hyun Kim

nlp.with.deep.learning@gmail.com




Review:ï¿½Autoencoders

â€¢ ì˜¤í† ì¸ì½”ë”(AE)ëŠ” ì••ì¶•ê³¼í•´ì œë¥¼ë°˜ë³µí•˜ë©°íŠ¹ì§•ì¶”ì¶œì„ìë™ìœ¼ë¡œí•™ìŠµ

â€¢ ì¸ì½”ë”ë¡œë¶€í„°ë‚˜ì˜¨ì¤‘ê°„ê²°ê³¼ë¬¼(ğ‘§)ì€ ì…ë ¥(ğ‘¥)ì— ëŒ€í•œ featureï¿½vectorì´ë‹¤.
â€¢ Motivation:ï¿½featureï¿½vectorì˜ ê°ì°¨ì›ì€ì–´ë–¤ì˜ë¯¸ë¥¼ë‚´í¬í•˜ê³ ìˆì„ê¹Œ?

ğ‘§ğ‘¥ #ğ‘¥Encoder Decoder
Bottleneck



Review:ï¿½Autoencoders

â€¢ ì¸ì½”ë”ì˜ê²°ê³¼ë¬¼ ğ‘§ë¥¼ plot í•˜ì˜€ì„ë•Œ, ë¹„ìŠ·í•œìƒ˜í”Œë“¤ì€ë¹„ìŠ·í•œê³³ì—ìœ„ì¹˜í•¨ì„í™•ì¸
â€¢ ì´ plotì´ ë¿Œë ¤ì§„ê³µê°„ì„ hidden(latent)ï¿½spaceë¼ê³ ë¶€ë¦„

â€¢ Inputï¿½spaceì˜ MNIST ìƒ˜í”Œì´ latent spaceì— embeddingï¿½ëœ ê²ƒ

ğ‘§ğ‘¥ Encoder



Mappingï¿½toï¿½Hidden(Latent)ï¿½Space

â€¢ ê°ë ˆì´ì–´ì˜ê²°ê³¼ë¬¼ì„ hiddenï¿½vectorë¼ê³ ë¶€ë¦„
â€¢ ëª¨ë‘ featureï¿½vectorë¼ê³ ë³¼ìˆ˜ìˆìŒ

ğ‘¦ğ‘¥ â„! â„"â„“! â„“" â„“#



Hidden(Latent)ï¿½Representation

â€¢ Tabularï¿½dataì˜ featureï¿½vectorì™€ ë‹¬ë¦¬, hiddenï¿½vectorëŠ” í•´ì„ì´ì–´ë ¤ì›€
â€¢ í•´ì„í•˜ê³ ìí•˜ëŠ”ì—°êµ¬ë“¤ì´ì´ì–´ì§€ê³ ìˆìœ¼ë‚˜, ì•„ì§ê°ˆê¸¸ì´ë©€ë‹¤.

â€¢ í•˜ì§€ë§Œ, ë¹„ìŠ·í•œíŠ¹ì§•ì„ê°€ì§„ìƒ˜í”Œì€ë¹„ìŠ·í•œ hiddenï¿½vectorë¥¼ ê°€ì§ˆê²ƒ!

ğ‘¦ğ‘¥ â„! â„"â„“! â„“" â„“#



Imageï¿½Generation

â€¢ ë§Œì•½ê°ì°¨ì›ì´ëª…í™•í•˜ê²Œí•˜ë‚˜ì˜ì˜ë¯¸ë¥¼ì§€ë‹Œë‹¤ë©´,
ê° ì°¨ì›ì˜ìˆ«ìë¥¼ì¡°ì ˆí•˜ì—¬ì›í•˜ëŠ”ì´ë¯¸ì§€ë¥¼í•©ì„±í•´ë‚¼ìˆ˜ìˆì„ê²ƒ.

ì¶œì²˜: https://towardsdatascience.com/understanding-latent-space-in-machine-learning-de5a7c687d8d



Summary

â€¢ Hiddenï¿½layerì˜ ê²°ê³¼ê°’ë“¤ì„ hidden vectorsë¼ ë¶€ë¥´ë©°,
ì´ë“¤ì€ìƒ˜í”Œì˜ featureë¥¼ ë‹´ê³ ìˆìŒ

â€¢ ì‹ ê²½ë§(ë˜ëŠ”ë ˆì´ì–´)ì„ í†µê³¼ì‹œí‚¤ëŠ”ê²ƒì€
ì…ë ¥ê³µê°„(inputï¿½space)ì—ì„œì ì¬ê³µê°„(latentï¿½space)ë¡œì˜ë§µí•‘ê³¼ì •

â€¢ ê³ ì°¨ì›ê³µê°„(high-dimensionalï¿½space)Ã ì €ì°¨ì›ê³µê°„(lower-dimensionalï¿½space)

â€¢ Hiddenï¿½representationì„ í•´ì„í•˜ëŠ”ê²ƒì€ë§¤ìš°ì–´ë ¤ì›€
â€¢ í•˜ì§€ë§Œë¹„ìŠ·í•œìƒ˜í”Œì€ë¹„ìŠ·í•œ hiddenï¿½representationì„ ì§€ë‹ê²ƒ!


