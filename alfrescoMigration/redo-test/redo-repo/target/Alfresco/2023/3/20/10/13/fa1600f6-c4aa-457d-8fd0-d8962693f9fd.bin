
KullbackÂ­Leibler
Divergence

Ki Hyun Kim

nlp.with.deep.learning@gmail.com




KullbackÂ­Leibler Divergence

â€¢ Measureï¿½dissimilarityï¿½ofï¿½twoï¿½probabilityï¿½distributions,ï¿½ğ‘ andï¿½ğ‘.
â€¢ Sinceï¿½KLDï¿½isï¿½asymmetric,ï¿½itï¿½isï¿½notï¿½aï¿½distance.KL ğ‘||ğ‘ = âˆ’ğ”¼!~# ! log ğ‘ ğ‘¥ğ‘ ğ‘¥= âˆ’-ğ‘ ğ‘¥ log ğ‘ ğ‘¥ğ‘ ğ‘¥ ğ‘‘ğ‘¥



DNNï¿½Optimizationï¿½usingï¿½KL-Divergence

â„’ ğœƒ = âˆ’ğ”¼!~# ! ğ”¼$~# $|! log ğ‘& ğ‘¦|ğ‘¥ğ‘ ğ‘¦|ğ‘¥
ğ· = ğ‘¥', ğ‘¦' '()*

byï¿½Monte-Carlo

â„’ ğœƒ â‰ˆ âˆ’ 1ğ‘ 5 ğ‘˜7'()* 7+(), log ğ‘& ğ‘¦',+|ğ‘¥'ğ‘ ğ‘¦',+|ğ‘¥'â‰ˆ âˆ’ 1ğ‘7'()* log ğ‘& ğ‘¦'|ğ‘¥'ğ‘ ğ‘¦'|ğ‘¥' , if ğ‘˜ = 1.



DNNï¿½Optimizationï¿½usingï¿½KL-Divergence

â„’ ğœƒ = âˆ’ 1ğ‘)!"#$ log ğ‘% ğ‘¦!|ğ‘¥!ğ‘ ğ‘¦!|ğ‘¥!0ğœƒ = argmin%âˆˆ' â„’ ğœƒğœƒ â† ğœƒ âˆ’ ğ›¼âˆ‡%â„’ ğœƒ


