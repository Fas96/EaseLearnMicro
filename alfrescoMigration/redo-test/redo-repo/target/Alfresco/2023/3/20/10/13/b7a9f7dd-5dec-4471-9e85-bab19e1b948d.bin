
Back-propagationï¿½
Throughï¿½Time

Ki Hyun Kim

nlp.with.deep.learning@gmail.com




Back-propagationï¿½withï¿½Multipleï¿½Forwardingï¿½Paths

ğœ•â„’ğœ•ğœƒ! = ğœ•â„’ğœ• %ğ‘¦ ğœ• %ğ‘¦ğœ•â„",! ğœ•â„",!ğœ•â„! + ğœ•%ğ‘¦ğœ•â„"," ğœ•â„","ğœ•â„! ğœ•â„!ğœ•ğœƒ!

ğ‘¥ %ğ‘¦â„!
â„",!

â„","
ğœƒ!



Back-propagationï¿½inï¿½Recurrentï¿½Neuralï¿½Networks

â€¢ Manyï¿½toï¿½One

â„! â„" â„# â„$ğ‘¥! ğ‘¥" ğ‘¥# ğ‘¥$
&ğ‘¦
ğ‘¦

â„%
â„’ ğœƒ = &ğ‘¦ âˆ’ ğ‘¦



Back-propagationï¿½inï¿½Recurrentï¿½Neuralï¿½Networks

â€¢ Manyï¿½toï¿½Many

â„! â„" â„# â„$ğ‘¥! ğ‘¥" ğ‘¥# ğ‘¥$
&ğ‘¦! &ğ‘¦" &ğ‘¦# &ğ‘¦$
ğ‘¦! ğ‘¦" ğ‘¦# ğ‘¦$

â„%
â„’ ğœƒ =,&'!( &ğ‘¦& âˆ’ ğ‘¦&



Back-propagationï¿½inï¿½Recurrentï¿½Neuralï¿½Networks

â„# â„$

ğ‘¥! ğ‘¥" ğ‘¥# ğ‘¥$

&ğ‘¦! &ğ‘¦" &ğ‘¦# &ğ‘¦$
ğ‘¦! ğ‘¦" ğ‘¦# ğ‘¦$

â„%
â„’ ğœƒ =,&'!( &ğ‘¦& âˆ’ ğ‘¦&

â„! â„"

â€¢ Manyï¿½toï¿½Manyï¿½withï¿½Multi-layeredï¿½RNN



TanH inï¿½Vanillaï¿½Recurrentï¿½Neuralï¿½Networks



Gradientï¿½Vanishing



Gradientï¿½Vanishing



Gradientï¿½Vanishing

Hard to train LONG sequence.



Summary

â€¢ ì—¬ëŸ¬ê²½ë¡œë¡œ feed-forwardï¿½ë  ê²½ìš°, back-propagation í•  ë•Œ,
ìµœì¢… gradient ê°’ì€ê°ê²½ë¡œì˜ gradientë“¤ì˜ì´í•©ì´ëœë‹¤.

â€¢ RNNì˜ê²½ìš°ì—ë„ê° time-stepì— ë”°ë¼ feed-forwardë˜ë¯€ë¡œ,
ê° ê²½ë¡œë¡œë¶€í„°ì „ë‹¬ë˜ì–´ì˜¨ gradientë“¤ì´ë”í•´ì§„ë‹¤.

â€¢ ë”°ë¼ì„œ time-stepì˜ ê°¯ìˆ˜ë§Œí¼ë ˆì´ì–´ê°€ê¹Šì–´ì§„ê²ƒê³¼ë§ˆì°¬ê°€ì§€ì´ë¯€ë¡œ,
tanhë¥¼ í™œì„±í•¨ìˆ˜ë¡œì‚¬ìš©í•˜ëŠ” RNNì€ gradientï¿½vanishingì´ ë°œìƒí•œë‹¤.


