
Back-propagation�
Through�Time

Ki Hyun Kim

nlp.with.deep.learning@gmail.com




Back-propagation�with�Multiple�Forwarding�Paths

𝜕ℒ𝜕𝜃! = 𝜕ℒ𝜕 %𝑦 𝜕 %𝑦𝜕ℎ",! 𝜕ℎ",!𝜕ℎ! + 𝜕%𝑦𝜕ℎ"," 𝜕ℎ","𝜕ℎ! 𝜕ℎ!𝜕𝜃!

𝑥 %𝑦ℎ!
ℎ",!

ℎ","
𝜃!



Back-propagation�in�Recurrent�Neural�Networks

• Many�to�One

ℎ! ℎ" ℎ# ℎ$𝑥! 𝑥" 𝑥# 𝑥$
&𝑦
𝑦

ℎ%
ℒ 𝜃 = &𝑦 − 𝑦



Back-propagation�in�Recurrent�Neural�Networks

• Many�to�Many

ℎ! ℎ" ℎ# ℎ$𝑥! 𝑥" 𝑥# 𝑥$
&𝑦! &𝑦" &𝑦# &𝑦$
𝑦! 𝑦" 𝑦# 𝑦$

ℎ%
ℒ 𝜃 =,&'!( &𝑦& − 𝑦&



Back-propagation�in�Recurrent�Neural�Networks

ℎ# ℎ$

𝑥! 𝑥" 𝑥# 𝑥$

&𝑦! &𝑦" &𝑦# &𝑦$
𝑦! 𝑦" 𝑦# 𝑦$

ℎ%
ℒ 𝜃 =,&'!( &𝑦& − 𝑦&

ℎ! ℎ"

• Many�to�Many�with�Multi-layered�RNN



TanH in�Vanilla�Recurrent�Neural�Networks



Gradient�Vanishing



Gradient�Vanishing



Gradient�Vanishing

Hard to train LONG sequence.



Summary

• 여러경로로 feed-forward�될 경우, back-propagation 할 때,
최종 gradient 값은각경로의 gradient들의총합이된다.

• RNN의경우에도각 time-step에 따라 feed-forward되므로,
각 경로로부터전달되어온 gradient들이더해진다.

• 따라서 time-step의 갯수만큼레이어가깊어진것과마찬가지이므로,
tanh를 활성함수로사용하는 RNN은 gradient�vanishing이 발생한다.


