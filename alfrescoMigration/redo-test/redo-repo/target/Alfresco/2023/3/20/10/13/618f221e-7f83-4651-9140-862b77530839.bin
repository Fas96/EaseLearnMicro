
RNN:
Step-by-Step

Ki Hyun Kim

nlp.with.deep.learning@gmail.com




Recurrent�Neural�Networks

𝑥! ℎ!



Recurrent�Neural�Networks�101

• 다른표현방법

𝑥! ℎ! ℎ! ℎ" ℎ# ℎ$𝑥! 𝑥" 𝑥# 𝑥$
ℎ! ℎ" ℎ# ℎ$



Recurrent�Neural�Network�101

• How�it�works.

ℎ! ℎ" ℎ# ℎ$𝑥! 𝑥" 𝑥# 𝑥$
#𝑦! #𝑦" #𝑦# #𝑦$
𝑦! 𝑦" 𝑦# 𝑦$

ℎ%
ℒ 𝜃 = (&'!( #𝑦& − 𝑦&



Zoom�In:�Input�Tensor

×𝑛



Zoom�In:�Hidden�Tensor

×𝑛
𝑦



Multi-layered�RNN

ℎ# ℎ$

𝑥! 𝑥" 𝑥# 𝑥$

#𝑦! #𝑦" #𝑦# #𝑦$
𝑦! 𝑦" 𝑦# 𝑦$

ℎ%
ℒ 𝜃 = (&'!( #𝑦& − 𝑦&

ℎ! ℎ"



Zoom�In:�Output�Tensor

ℎ# ℎ$

𝑥! 𝑥" 𝑥# 𝑥$

#𝑦! #𝑦" #𝑦# #𝑦$
𝑦! 𝑦" 𝑦# 𝑦$

ℎ%
ℒ 𝜃 = (&'!( #𝑦& − 𝑦&

ℎ! ℎ"



Zoom�In:�Hidden�State�Tensor

ℎ& = #𝑙𝑎𝑦𝑒𝑟𝑠, 𝑏𝑎𝑡𝑐ℎ_𝑠𝑖𝑧𝑒, ℎ𝑖𝑑𝑑𝑒𝑛_𝑠𝑖𝑧𝑒
ℎ#

𝑥! 𝑥" 𝑥# 𝑥$

#𝑦! #𝑦" #𝑦# #𝑦$
𝑦! 𝑦" 𝑦# 𝑦$

ℎ%
ℒ 𝜃 = (&'!( #𝑦& − 𝑦&

ℎ! ℎ" ℎ$



Zoom�In:�Input�Tensor

ℎ# ℎ$

𝑥! 𝑥" 𝑥# 𝑥$

#𝑦! #𝑦" #𝑦# #𝑦$
𝑦! 𝑦" 𝑦# 𝑦$

ℎ%
ℒ 𝜃 = (&'!( #𝑦& − 𝑦&

ℎ! ℎ"
Same�shape�as�before



Bidirectional�Multi-layered�RNN

𝑥! 𝑥" 𝑥# 𝑥$

#𝑦! #𝑦" #𝑦# #𝑦$
𝑦! 𝑦" 𝑦# 𝑦$

ℎ%

ℒ 𝜃 = (&'!( #𝑦& − 𝑦&

ℎ%



Zoom�In:�Output�Tensor

ℎ!:* = 𝑏𝑎𝑡𝑐ℎ_𝑠𝑖𝑧𝑒, 𝑛, ℎ𝑖𝑑𝑑𝑒𝑛_𝑠𝑖𝑧𝑒×#𝑑𝑖𝑟𝑒𝑐𝑡𝑖𝑜𝑛𝑠!! !" !# !$

"#! "#" "## "#$
"! "" "# "$

ℎ%

ℒ & =( "#& − "&'&(!
ℎ%



Zoom�In:�Hidden�State�Tensor

!! !" !# !$

"#! "#" "## "#$
"! "" "# "$

ℎ%

ℒ & =( "#& − "&'&(!
ℎ%



You�can�refer�documents,�anytime.

Input Shape Output Shape



Summary

• Single�layer�RNN에서 hidden�state는 곧 output이다.

• Multi-layered�RNN에서
• Output은 마지막 layer의 모든 time-step의 hidden�state이다.
• Hidden�state는 마지막 time-step의 모든 layer의 hidden�state이다.

• Bi-directional�RNN에서
• Output은 hidden�state가 2배가된다.
• Hidden�state는 layer의 갯수가 2배가된다.


