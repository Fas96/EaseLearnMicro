
Popular�Backbone:
ResNet

Ki Hyun Kim

nlp.with.deep.learning@gmail.com




Introduction

• [He�et�al.,�2015]�Deep�Residual�Learning�for�Image�Recognition
• [He�et�al.,�2016] Identity�Mappings�in�Deep�Residual�Networks

Revolution�of�Depth
출처: http://kaiminghe.com/icml16tutorial/index.html




Motivations

• ImageNet�대회가거듭될수록, 깊은네트워크가우승을차지함

• 깊은네트워크를학습시키는데애로사항이많음
• 최적화문제: Training�loss가 잘낮아지지않음

• 최적의깊이가존재할텐데, 깊어지면나머지는 identity�함수면될것아닌가?

출처: http://kaiminghe.com/icml16tutorial/index.html




Methodology

• 𝐹 𝑥 = 𝐻 𝑥 − 𝑥
• 𝐻 𝑥 = 𝐹(𝑥) + 𝑥

𝐹 𝑥

𝐻 𝑥



Methodology

• Residual�Block을 쌓자



Evaluation

• 기존:
• 레이어가깊어질수록낮은성능

• Resnet:
• 레이어가깊어질수록높은성능

Training�error�(dashed�lines)�&�Test�error�(bold�lines)�on�CIFAR-10



Later,�it�turns�out

• Resnet은 gradient�vanishing을 방지하는방법

• 다른 gradient�vanishing�방지방법
• Highway�Networks�[Srivastava�et�al.,�2015]
• Linear�Gated�Unit�[Dauphin�et�al.,�2016]

• 현재제안되는대부분의큰네트워크들은 residual�connection을 차용
• e.g.�Transformer


